{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5ad08a",
   "metadata": {},
   "source": [
    "# Cleaning the German-English Language Sentence Pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7de4af",
   "metadata": {},
   "source": [
    "This code is refereneced from https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/ </br> \n",
    "and https://github.com/zzc01/Transformer/blob/main/Data_Cleaning/clean_pairs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a83bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump, load \n",
    "from unicodedata import normalize \n",
    "from numpy import array \n",
    "from numpy.random import shuffle\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173ddeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01000cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [[l.split('\\t')[1], l.split('\\t')[0]] for l in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2b5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pairs(pariedLines):\n",
    "    cleaned = list()\n",
    "    # remove non-printable chars\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in pariedLines:\n",
    "        clean_pair = list()\n",
    "        for sentence in pair:\n",
    "            sentence = normalize('NFD', sentence).encode('ascii', 'ignore')\n",
    "            sentence = sentence.decode('UTF-8')\n",
    "            sentence = sentence.split()\n",
    "            sentence = [word.lower() for word in sentence]\n",
    "            sentence = [word.translate(table) for word in sentence]\n",
    "            # remove non-printable chars\n",
    "            sentence = [re_print.sub('', word) for word in sentence]\n",
    "            # Remove words with numbers? how to deal with numbers? \n",
    "            # How to deal with upper case? And , . ? % these signs? \n",
    "            sentence = [word for word in sentence if word.isalpha()] \n",
    "            clean_pair.append(' '.join(sentence))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de69817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print(f'Saved: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ed0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of setences = 261499\n",
      "max_sentence_len = 75, 101\n",
      "Saved: ./data/german-english.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './data/deu.txt'\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "cleaned_pairs = clean_pairs(pairs)\n",
    "print(f'number of setences = {len(cleaned_pairs)}')\n",
    "print(f'max_sentence_len = {len(cleaned_pairs[-1][0].split())}, {len(cleaned_pairs[-1][1].split())}')        \n",
    "save_clean_data(cleaned_pairs, './data/german-english.pkl')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8611b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[geh] -> [go]\n",
      "[hallo] -> [hi]\n",
      "[gru gott] -> [hi]\n",
      "[lauf] -> [run]\n",
      "[lauf] -> [run]\n",
      "[lass uns nach hause gehen] -> [let us go home]\n",
      "[lasst uns nach hause gehen] -> [let us go home]\n",
      "[lasst uns mutig sein] -> [lets be brave]\n",
      "[rufen wir tom an] -> [lets call tom]\n",
      "[lasst uns weitermachen] -> [lets continue]\n",
      "[ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht dass ein mensch nur gelegenheit hat mit ein paar hundert anderen bekannt zu sein von denen ihm nur ein dutzend oder weniger nahesteht darunter hochstens ein oder zwei freunde dann erahnt man eingedenk der millionen einwohner dieser weltleicht dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist] -> [doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people and out of the few hundred that there are but a dozen or less whom he knows intimately and out of the dozen one or two friends at most it will easily be seen when we remember the number of millions who inhabit this world that probably since the earth was created the right man has never yet met the right woman]\n",
      "[ich wei wohl dass das ausschlieliche beitragen von satzen in der muttersprache oder der am besten beherrschten sprache nicht ganz so viel spa macht wie sich im schreiben von fremdsprachen zu uben steuere beim tatoebakorpus aber bitte trotzdem keine satze bei uber deren korrektheit du dir nicht vollig im klaren bist wenn du sprachen die du gerade lernst uben mochtest verwende dazu bitte netzangebote die eigens hierfur eingerichtet wurden wie zum beispiel] -> [i know that adding sentences only in your native or strongest language is probably not as much fun as practicing writing foreign languages but please dont add sentences to the tatoeba corpus if you are not absolutely sure they are correct if you want to practice languages that you are studying please do so by using a website designed for that purpose such as]\n",
      "[es ist wohl unmoglich einen vollkommen fehlerfreien korpus zu erreichen das liegt in der natur eines solchen gemeinschaftsprojekts doch wenn wir unsere mitglieder dazu bringen konnen nicht mit sprachen herumzuexperimentieren die sie gerade lernen sondern satze in ihrer eigenen muttersprache beizutragen dann gelingt es uns vielleicht die zahl der fehler klein zu halten] -> [it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors]\n",
      "[wenn jemand der nicht wei woher man kommt sagt man erwecke doch den eindruck muttersprachler zu sein so hat man grund zu der annahme dass ihm an der sprache irgendetwas aufgefallen ist woran er erkannt hat dass man eben keiner ist dass man diesen eindruck mit anderen worten eigentlich nicht erweckt] -> [if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker]\n",
      "[wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an] -> [if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):    print('[%s] -> [%s]' % (cleaned_pairs[0+i,0], cleaned_pairs[0+i,1]))\n",
    "for i in range(5):    print('[%s] -> [%s]' % (cleaned_pairs[10000+i,0], cleaned_pairs[10000+i,1]))\n",
    "for i in range(5):    print('[%s] -> [%s]' % (cleaned_pairs[-i-1,0], cleaned_pairs[-i-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cde1b0",
   "metadata": {},
   "source": [
    "## Split the data into training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69762d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab32736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print(f'Saved: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "288f62ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_clean_sentences('./data/german-english.pkl')\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "538fd32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "n_sentences = 10000\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "# test_ratio = 0.1\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "shuffle(dataset)\n",
    "train = dataset[ : int(n_sentences*train_ratio)]\n",
    "val   = dataset[int(n_sentences*train_ratio):int(n_sentences*(train_ratio+val_ratio))]\n",
    "test  = dataset[int(n_sentences*(train_ratio+val_ratio)):]\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945fc11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['komm her' 'get over here']\n",
      "['ich habe verloren' 'ive lost']\n",
      "['tom wurde adoptiert' 'toms adopted']\n",
      "['tom liebt rum' 'tom likes rum']\n",
      "['tom kannte ihn' 'tom knew it']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): print(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba56e229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sie hasste ihn' 'she hated him']\n",
      "['ich rieche kaffee' 'i smell coffee']\n",
      "['ach sei still' 'oh be quiet']\n",
      "['mir tut der kiefer weh' 'my jaw hurts']\n",
      "['du bist ja stark' 'youre strong']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): print(val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47da6019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom erbricht sich' 'tom is puking']\n",
      "['welche gehort uns' 'which is ours']\n",
      "['ich bin arzt' 'im a medic']\n",
      "['scher dich fort' 'get away']\n",
      "['er legte auf' 'he hung up']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): print(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e708649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./data/german-english-both.pkl\n",
      "Saved: ./data/german-english-train.pkl\n",
      "Saved: ./data/german-english-val.pkl\n",
      "Saved: ./data/german-english-test.pkl\n"
     ]
    }
   ],
   "source": [
    "save_clean_data(dataset, './data/german-english-both.pkl')\n",
    "save_clean_data(train, './data/german-english-train.pkl')\n",
    "save_clean_data(val, './data/german-english-val.pkl')\n",
    "save_clean_data(test, './data/german-english-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3dd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/german-english-test.pkl', 'rb') as f:\n",
    "    text = load(f)\n",
    "print(len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ced2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/german-english-test.txt', 'wb') as f:\n",
    "    for pair in text:\n",
    "        word = pair[0]+'\\t'+pair[1] + '\\n'\n",
    "        f.write(word.encode('utf-8'))\n",
    "f.close()\n",
    "    \n",
    "# savetxt('./data/german-english-test.txt', text, fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
