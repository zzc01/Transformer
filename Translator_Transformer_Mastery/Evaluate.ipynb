{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsd10k9mX64O"
   },
   "source": [
    "# **Config and Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1688862117460,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "QUMG_XSFOak7"
   },
   "outputs": [],
   "source": [
    "# For Colab. Need to run this after run time reset\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/Colab Notebooks/transformer_mastery')\n",
    "# path = '/content/drive/MyDrive/Colab Notebooks/transformer_mastery/metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5001,
     "status": "ok",
     "timestamp": 1688862122726,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "Jasb0oHPa8C8",
    "outputId": "20e4e6ab-e966-4d9d-b427-80388676d1f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.22.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
      "Installing collected packages: keras_preprocessing\n",
      "Successfully installed keras_preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# %pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11473,
     "status": "ok",
     "timestamp": 1688862134197,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "u28iYG9d9Jkf"
   },
   "outputs": [],
   "source": [
    "from transformer import TransformerModel\n",
    "from pickle import load\n",
    "from tensorflow import Module\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64, TensorArray, argmax, newaxis, transpose\n",
    "from time import time\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3v5CtWt_F7e"
   },
   "source": [
    "# **Load data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1688862134622,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "pXO3FLTR_GXu",
    "outputId": "390cd716-d9b3-46fa-99d9-76caa06b2b63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"./data/german-english-test.txt\"\n",
    "with open(filename, 'rt') as file:\n",
    "  test_data = file.read()\n",
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom erbricht sich', 'tom is puking']\n",
      "['welche gehort uns', 'which is ours']\n",
      "['ich bin arzt', 'im a medic']\n",
      "['scher dich fort', 'get away']\n",
      "['er legte auf', 'he hung up']\n"
     ]
    }
   ],
   "source": [
    "test_data2 = test_data.split('\\n')\n",
    "test_data3 = [text.split('\\t') for text in test_data2]\n",
    "test_data3 = test_data3[:-1]\n",
    "for i in range(5): print(test_data3[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLCRydl8OWFk"
   },
   "source": [
    "# **Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1688862134872,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "I91Bqfk69TLX"
   },
   "outputs": [],
   "source": [
    "h = 8\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n = 6\n",
    "\n",
    "#9 7 3157 2009\n",
    "enc_seq_length = 9\n",
    "dec_seq_length = 7\n",
    "enc_vocab_size = 3157\n",
    "dec_vocab_size = 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 31379,
     "status": "ok",
     "timestamp": 1688862166248,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "X_HZDx_g9TJO"
   },
   "outputs": [],
   "source": [
    "inferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1688864403061,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "WnHo9lwY9TG2"
   },
   "outputs": [],
   "source": [
    "class Translate(Module):\n",
    "    def __init__(self, inference_model, **kwargs):\n",
    "        super(Translate, self).__init__(**kwargs)\n",
    "        self.transformer = inference_model\n",
    "\n",
    "    def load_tokenizer(self, name):\n",
    "        with open(name, 'rb') as handle:\n",
    "            return load(handle)\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"\n",
    "\n",
    "        enc_tokenizer = self.load_tokenizer('./metadata/enc_tokenizer.pkl')\n",
    "        dec_tokenizer = self.load_tokenizer('./metadata/dec_tokenizer.pkl')\n",
    "\n",
    "        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n",
    "        encoder_input = pad_sequences(encoder_input, maxlen=enc_seq_length, padding='post')\n",
    "        encoder_input = convert_to_tensor(encoder_input, dtype=int64)\n",
    "\n",
    "        output_start = dec_tokenizer.texts_to_sequences([\"<START>\"])\n",
    "        output_start = convert_to_tensor(output_start[0], dtype=int64)\n",
    "\n",
    "        output_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])\n",
    "        output_end = convert_to_tensor(output_end[0], dtype=int64)\n",
    "\n",
    "        decoder_output = TensorArray(dtype=int64, size=0, dynamic_size=True)\n",
    "        decoder_output = decoder_output.write(0, output_start)\n",
    "\n",
    "        for i in range(dec_seq_length):\n",
    "            prediction = self.transformer(encoder_input, transpose(decoder_output.stack()), training=False)\n",
    "            prediction = prediction[:, -1, :] # pick the last prediction\n",
    "            predicted_id = argmax(prediction, axis=-1)\n",
    "            decoder_output = decoder_output.write(i+1, predicted_id)\n",
    "\n",
    "            if predicted_id == output_end:\n",
    "                break\n",
    "\n",
    "        output = transpose(decoder_output.stack())[0]\n",
    "        output = output.numpy()\n",
    "        # print(f\"output = {output}\")\n",
    "\n",
    "        output_str = []\n",
    "        for i in range(output.shape[0]):\n",
    "            key = output[i]\n",
    "            if key == 0:\n",
    "              output_str.append(dec_tokenizer.index_word[2])  # to fix the attend to 0 problem\n",
    "            else:\n",
    "              output_str.append(dec_tokenizer.index_word[key])\n",
    "\n",
    "\n",
    "        output_str = output_str[1:-1]\n",
    "        return ' '.join(output_str)\n",
    "\n",
    "inferencing_model.load_weights('./metadata/weights/wghts20.ckpt')\n",
    "translator = Translate(inferencing_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCjZ9arPio9E"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2611,
     "status": "ok",
     "timestamp": 1688864045139,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "8ct8cjHg9TEu",
    "outputId": "c78d7546-07c4-4dd3-db50-8b18f62e21a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated result is =  i let a smoke\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"er legte auf\"]\n",
    "print('Translated result is = ', translator(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28UJnGi7h7Ac"
   },
   "source": [
    "# **BLEU Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2363064,
     "status": "ok",
     "timestamp": 1688866780976,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "NPuYyZ2Fh8rr",
    "outputId": "28959d2f-d289-4eb4-eee5-ac50b8b61f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = tom erbricht sich\n",
      "target = tom is puking\n",
      "predict = i can swim\n",
      "\n",
      "\n",
      "src = welche gehort uns\n",
      "target = which is ours\n",
      "predict = i prefer why\n",
      "\n",
      "\n",
      "src = ich bin arzt\n",
      "target = im a medic\n",
      "predict = im a doctor\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "time0 = time()\n",
    "length = 0\n",
    "for i, source in enumerate(test_data3):\n",
    "    raw_src = source[0]\n",
    "    raw_target = source[1]\n",
    "    # if i == 400: break\n",
    "    #\n",
    "    translation = translator([raw_src])\n",
    "    if i < 3:\n",
    "        print(f\"src = {raw_src}\")\n",
    "        print(f\"target = {raw_target}\")\n",
    "        print(f\"predict = {translation}\")\n",
    "        print(\"\\n\")\n",
    "    # #\n",
    "    actual.append([raw_target.split()])\n",
    "    predicted.append(translation.split())\n",
    "\n",
    "    length += 1\n",
    "    if length % 200 ==0:\n",
    "        print(f\"length = {length}, run time = {time()-time0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1688869705069,
     "user": {
      "displayName": "ZUOW-ZUN CHEN",
      "userId": "13156931386379130288"
     },
     "user_tz": 420
    },
    "id": "XXiVSDnDh9Sx",
    "outputId": "8e478b87-4799-4ba1-9cfd-86fc057c6777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict time = 2123.378015756607\n",
      "BLEU-1 0.263259\n",
      "BLEU-2 0.162598\n",
      "BLEU-3 0.105114\n",
      "BLEU-4 0.054208\n",
      "BLEU time = 2123.4946296215057\n"
     ]
    }
   ],
   "source": [
    "print(f'Predict time = {time()-time0}')\n",
    "print('BLEU-1 %f' % corpus_bleu(actual, predicted, weights=(1.0, 0.0, 0.0, 0.0)))\n",
    "print('BLEU-2 %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0.0, 0.0)))\n",
    "print('BLEU-3 %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0.0)))\n",
    "print('BLEU-4 %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(f'BLEU time = {time()-time0}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMmiiKif2QUJA6VR6Ca7Mo",
   "mount_file_id": "11yBCitM8eMg01nPJ-M1kFkcVXDTQNCnu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
