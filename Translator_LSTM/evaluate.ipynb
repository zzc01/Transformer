{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3090ee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 19:53:14.308497: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 19:53:14.464908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-17 19:53:14.464924: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-17 19:53:14.965616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-17 19:53:14.965731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-17 19:53:14.965736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model \n",
    "from nltk.translate.bleu_score import corpus_bleu \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f870c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4415665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdd9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b487093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'National', 'Basketball', 'Association', '(NBA)', 'is', 'a', 'professional', 'basketball', 'league', 'in', 'North', 'America', 'composed', 'of', '30', 'teams']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "lines = ['The National Basketball Association (NBA) is a professional basketball league in North America composed of 30 teams', 'It is one of the major professional sports leagues in the United States and Canada and is considered the premier professional basketball league in the world.']\n",
    "print(lines[0].split())\n",
    "print(max_length(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee83953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014d9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_idx(idx, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == idx:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43b1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_clean_sentences('../Data_Cleaning/english-german-both.pkl')\n",
    "train = load_clean_sentences('../Data_Cleaning/english-german-train.pkl')\n",
    "test = load_clean_sentences('../Data_Cleaning/english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc9ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2166 5\n",
      "3539 8\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print(eng_vocab_size, eng_length)\n",
    "#\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print(ger_vocab_size, ger_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9721fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:,1])\n",
    "testX  = encode_sequences(ger_tokenizer, ger_length, test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775152e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ger_length = 8\n",
      "(9000, 8)\n",
      "(1000, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f'ger_length = {ger_length}')\n",
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0522fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 63 26  0  0  0  0  0]\n",
      "[[ 2 63 26  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "source = trainX[0]\n",
    "print(source)\n",
    "source = source.reshape((1, source.shape[0]))\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52aa3125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 19:53:16.175636: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-17 19:53:16.175658: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-17 19:53:16.175673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (joseph-VirtualBox): /proc/driver/nvidia/version does not exist\n",
      "2023-06-17 19:53:16.175863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f27034eab00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f27034eab00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "src=[tom will eine], target=[tom wants one], predict=[tom wants one]\n",
      "src=[nimm meine], target=[take mine], predict=[take mine]\n",
      "src=[sie wollen mich], target=[they want me], predict=[they want me]\n",
      "src=[jetzt habe ich es begriffen], target=[i get it now], predict=[i do it now]\n",
      "src=[fahr sicher], target=[drive safely], predict=[drive safely]\n",
      "src=[wo ist er], target=[where is it], predict=[where is it]\n",
      "src=[ist er tom], target=[is he tom], predict=[is he tom]\n",
      "src=[niemand hat geschlafen], target=[nobody slept], predict=[nobody slept]\n",
      "src=[tanzen sie weiter], target=[keep dancing], predict=[keep dancing]\n",
      "src=[ich bin nicht zu fu gegangen], target=[i didnt walk], predict=[i didnt walk]\n",
      "Predict time = 38.18780469894409\n",
      "BLEU-1 0.860575\n",
      "BLEU-2 0.802822\n",
      "BLEU-3 0.664038\n",
      "BLEU-4 0.366201\n",
      "BLEU time = 38.324405670166016\n"
     ]
    }
   ],
   "source": [
    "def predict_sequence(mode, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    indicies = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for idx in indicies:\n",
    "        word = word_from_idx(idx, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    time0 = time()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predict=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()]) \n",
    "        predicted.append(translation.split())\n",
    "        if i == 1000:\n",
    "            break\n",
    "             \n",
    "    print(f'Predict time = {time()-time0}')\n",
    "    print('BLEU-1 %f' % corpus_bleu(actual, predicted, weights=(1.0, 0.0, 0.0, 0.0)))\n",
    "    print('BLEU-2 %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0.0, 0.0)))    \n",
    "    print('BLEU-3 %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0.0))) \n",
    "    print('BLEU-4 %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))  \n",
    "    print(f'BLEU time = {time()-time0}')\n",
    "    \n",
    "    \n",
    "model = load_model('model_1.h5')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60db3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[was fur eine niete], target=[what a loser], predict=[what a nerve]\n",
      "src=[horen sie auf zu lacheln], target=[stop smiling], predict=[stop smiling]\n",
      "src=[die vogel legen eier], target=[birds lay eggs], predict=[the cup]\n",
      "src=[tom war wutend], target=[tom was angry], predict=[tom was angry]\n",
      "src=[sie sind wach], target=[theyre awake], predict=[theyre awake]\n",
      "src=[er schoss auf mich], target=[he shot at me], predict=[he has me]\n",
      "src=[tom wird laufen], target=[tom will walk], predict=[tom will sing]\n",
      "src=[ich habe zu tun], target=[im busy], predict=[im not busy]\n",
      "src=[tom liegt im sterben], target=[tom is dying], predict=[toms dying]\n",
      "src=[ich habe ferien], target=[im on holiday], predict=[i have up]\n",
      "Predict time = 40.52018761634827\n",
      "BLEU-1 0.544372\n",
      "BLEU-2 0.410727\n",
      "BLEU-3 0.283821\n",
      "BLEU-4 0.120561\n",
      "BLEU time = 40.6274197101593\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c072eed",
   "metadata": {},
   "source": [
    "### The BLEU score usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463735e",
   "metadata": {},
   "source": [
    "reference = list of sentences, where a sentence is a list of tokens <br>\n",
    "cadidate  = list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15734367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3306e",
   "metadata": {},
   "source": [
    "reference = list of documents, where a document is a list of sentences <br>\n",
    "cadidate = list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd4021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# two references for one document\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is', 'a', 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab339228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['i', 'didnt', 'walk', 'home']]]\n",
      "[['i', 'didnt', 'walk', 'home']]\n",
      "BLEU-1 1.000000\n",
      "BLEU-2 1.000000\n",
      "BLEU-3 1.000000\n",
      "BLEU-4 1.000000\n"
     ]
    }
   ],
   "source": [
    "actual = [[\"i didnt walk home\".split()]]\n",
    "predicted = [\"i didnt walk home\".split()]\n",
    "print(actual)\n",
    "print(predicted)\n",
    "print('BLEU-1 %f' % corpus_bleu(actual, predicted, weights=(1.0, 0.0, 0.0, 0.0)))\n",
    "print('BLEU-2 %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))    \n",
    "print('BLEU-3 %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0))) \n",
    "print('BLEU-4 %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
