{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d350a21",
   "metadata": {},
   "source": [
    "# Evluate the translator using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058cef0",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f3131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "from pickle import load, dump\n",
    "from time import time \n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175d6cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fdfc9",
   "metadata": {},
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb36443",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking up text, into \"tokens\". Depending on the tokenizer, these tokens can represent sentence-pieces, words, subwords, or characters. To learn more about tokenization, visit this guide.\n",
    "\n",
    "This tutorial uses the tokenizers built in the subword tokenizer tutorial. That tutorial optimizes two text.BertTokenizer objects (one for English, one for Portuguese) for this dataset and exports them in a TensorFlow saved_model format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f8166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>\n",
      "<class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>\n"
     ]
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load('./Bootcamp/Tranformer_TF/deu-eng/metadata/tokenizer_deu_eng_1')\n",
    "string = \"When writing a sentence, generally you start with a capital letter and finish with a period (.), an exclamation mark (!), or a question mark (?).\"\n",
    "tokens = reloaded_tokenizers.eng.tokenize([string])\n",
    "round_trip = reloaded_tokenizers.eng.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdfe4e",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd06500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom zahlt ein Bußgeld.\n",
      "b'Tom is paying a fine.'\n",
      "\n",
      "\n",
      "Unter dem Bett ist eine Katze.\n",
      "b'There is a cat under the bed.'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = './Bootcamp/Tranformer_TF/deu-eng/data/deu-eng-test.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    test_data = load(file)\n",
    "type(test_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6868c9",
   "metadata": {},
   "source": [
    "# Load Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0fa1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = tf.saved_model.load('./Bootcamp/Tranformer_TF/deu-eng/metadata/translator_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "954f926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, _, _ = translator('Wir bekommen ein neues Auto nächsten Monat.')\n",
    "print(translation.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "raw_source, raw_target = test_data[idx][0].decode('utf-8'), test_data[idx][1].decode('utf-8')\n",
    "translation, _, _ = translator(raw_source)\n",
    "print(f\"src={raw_source}\")\n",
    "print(f\"target={raw_target}\")\n",
    "print(f\"predict={translation.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = tf.constant(raw_source)\n",
    "assert isinstance(sentence, tf.Tensor)\n",
    "print(sentence.shape)\n",
    "# sentence = sentence[tf.newaxis]\n",
    "print(sentence.shape)\n",
    "print(sentence.numpy())\n",
    "#\n",
    "translation, _, _ = translator(sentence)\n",
    "print(f\"src={sentence}\")\n",
    "print(f\"target={raw_target}\")\n",
    "print(f\"predict={translation.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5695ff",
   "metadata": {},
   "source": [
    "# BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1356a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"When writing a sentence, generally you start with a capital letter and finish with a period (.), an exclamation mark (!), or a question mark (?).\"\n",
    "target = reloaded_tokenizers.eng.tokenize([target])\n",
    "target = reloaded_tokenizers.eng.detokenize(target)\n",
    "target = target.numpy()[0].decode('utf-8')\n",
    "# print(target)\n",
    "actual = [[target.split()]]\n",
    "print(actual)\n",
    "\n",
    "predict = \"When writing a sentence, generally you start with a capital letter and finish with a period (.), an exclamation mark (!), or a question mark (?).\"\n",
    "predict = reloaded_tokenizers.eng.tokenize([predict])\n",
    "predict = reloaded_tokenizers.eng.detokenize(predict)\n",
    "predict = predict.numpy()[0].decode('utf-8')\n",
    "# print(predict)\n",
    "predicted = [predict.split()]\n",
    "print(predicted)\n",
    "\n",
    "print('BLEU-1    %f' % corpus_bleu(actual, predicted, weights=(1.0, 0.0, 0.0, 0.0)))\n",
    "print('BLEU-2    %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0.0, 0.0)))    \n",
    "print('BLEU-3    %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0.0))) \n",
    "print('BLEU-4    %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fba675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))\n",
    "print(len(test_data)*0.07/60, 'min to predict all the test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = list(), list()\n",
    "BLEU1, BLEU2, BLEU3, BLEU4, length = 0, 0, 0, 0, 0\n",
    "f = open( \"./Bootcamp/Tranformer_TF/deu-eng/metadata/eng_processed.txt\", 'rt')\n",
    "texts = f.read()\n",
    "f.close()\n",
    "texts = texts.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ca039",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time()\n",
    "\n",
    "for i, source in enumerate(test_data):\n",
    "    raw_src = source[0].decode('utf-8')\n",
    "    # raw_target = source[1].decode('utf-8')\n",
    "    raw_target = texts[i]\n",
    "    # if i == 100: break\n",
    "    #\n",
    "    translation, _, _ = translator(raw_src)\n",
    "    translation = translation.numpy().decode('utf-8')\n",
    "    if i < 3: \n",
    "        print(f\"src = {raw_src}\")\n",
    "        print(f\"target = {raw_target}\")\n",
    "        print(f\"predict = {translation}\")\n",
    "        print(\"\\n\")\n",
    "    #\n",
    "    actual.append([raw_target.split()])\n",
    "    predicted.append(translation.split())\n",
    "    \n",
    "    length += 1\n",
    "    if length % 200 ==0:\n",
    "        print(length, time()-time0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1db8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Predict time = {time()-time0}')\n",
    "print('BLEU-1 %f' % corpus_bleu(actual, predicted, weights=(1.0, 0.0, 0.0, 0.0)))\n",
    "print('BLEU-2 %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0.0, 0.0)))    \n",
    "print('BLEU-3 %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0.0))) \n",
    "print('BLEU-4 %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))  \n",
    "print(f'BLEU time = {time()-time0}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
