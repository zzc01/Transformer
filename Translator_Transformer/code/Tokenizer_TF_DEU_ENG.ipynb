{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "sw6cTrwKLtWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tensorflow_datasets is needed to download data from google cloud storage\n",
        "* tensorflow_text is needed in the tokenizer library"
      ],
      "metadata": {
        "id": "Ge3c34n7ekNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrVk3Z2odyp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a3ade2-1c1c-4259-cabe-3a9ba033477b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/6.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/6.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q tensorflow_datasets\n",
        "%pip install -q tensorflow_text tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tftxt\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow_datasets as tfds\n",
        "from pickle import dump, load\n",
        "import numpy as np\n",
        "from numpy.random import rand, shuffle\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/transformer_tf')\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/transformer_tf/deu-eng/metadata/'"
      ],
      "metadata": {
        "id": "G6_SGjB5fARd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathlib.Path.cwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yOiAgGWexo0",
        "outputId": "18874873-5afc-49e8-a039-79676a703441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "bhtZAKYjKmQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the data pairs"
      ],
      "metadata": {
        "id": "muF2vIFkAdtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'deu.txt'\n",
        "with open(path + filename, 'rt', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "p2ZoBc8tWIi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k87y0fpW1AtD",
        "outputId": "3e299802-d574-48a6-8b21-5e1a67aefee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = text.strip().split('\\n')\n",
        "pairs = [l.split('\\t')[0:2] for l in lines]\n",
        "for i, p in enumerate(pairs):\n",
        "  if i ==10: break\n",
        "  print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o1TiuUM1Aq6",
        "outputId": "c40881b1-2130-4ef8-a26e-422515b955c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.', 'Geh.']\n",
            "['Hi.', 'Hallo!']\n",
            "['Hi.', 'Grüß Gott!']\n",
            "['Run!', 'Lauf!']\n",
            "['Run.', 'Lauf!']\n",
            "['Wow!', 'Potzdonner!']\n",
            "['Wow!', 'Donnerwetter!']\n",
            "['Duck!', 'Kopf runter!']\n",
            "['Fire!', 'Feuer!']\n",
            "['Help!', 'Hilfe!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = list()\n",
        "for i, pair in enumerate(pairs):\n",
        "  clean_pair = list()\n",
        "  for sentence in pair:\n",
        "    sentence = normalize('NFD', sentence)\n",
        "    sentence = sentence.encode('utf-8')\n",
        "    clean_pair.append(sentence)\n",
        "  cleaned.append(clean_pair[::-1])\n",
        "\n",
        "for i, p in enumerate(cleaned):\n",
        "  if i ==10: break\n",
        "  print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHzBQb_s1Aos",
        "outputId": "3483cad4-3fd3-4df7-819d-46b04b5ed2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'Geh.', b'Go.']\n",
            "[b'Hallo!', b'Hi.']\n",
            "[b'Gru\\xcc\\x88\\xc3\\x9f Gott!', b'Hi.']\n",
            "[b'Lauf!', b'Run!']\n",
            "[b'Lauf!', b'Run.']\n",
            "[b'Potzdonner!', b'Wow!']\n",
            "[b'Donnerwetter!', b'Wow!']\n",
            "[b'Kopf runter!', b'Duck!']\n",
            "[b'Feuer!', b'Fire!']\n",
            "[b'Hilfe!', b'Help!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_sentence = len(cleaned)\n",
        "print(n_sentence)\n",
        "train_ratio = 0.8\n",
        "val_ratio   = 0.1\n",
        "test_ratio  = 0.1\n",
        "dataset = np.array(cleaned)\n",
        "shuffle(dataset)\n",
        "\n",
        "# dataset = cleaned[:10]\n",
        "\n",
        "for i, p in enumerate(dataset):\n",
        "  if i ==10: break\n",
        "  print(p)\n",
        "\n",
        "train = dataset[ : int(n_sentence*train_ratio)]\n",
        "val = dataset[int(n_sentence*train_ratio) : int(n_sentence*(train_ratio+val_ratio))]\n",
        "test = dataset[int(n_sentence*(train_ratio+val_ratio)) : int(n_sentence*(train_ratio+val_ratio+test_ratio))]\n",
        "\n",
        "print(len(train), len(val), len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1c_45Nf1Ak6",
        "outputId": "d12b7090-6ca2-45b6-9fe2-15b0f859a2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "261499\n",
            "[b'Ich bekomme diesen Splitter nicht aus meinem Finger.'\n",
            " b\"I can't get this splinter out of my finger.\"]\n",
            "[b'Diese Entscheidung hat Tom getroffen.'\n",
            " b'Tom was the one who made that decision.']\n",
            "[b'Tom wurde ermordet.' b'Tom has been murdered.']\n",
            "[b'Tom sagte, er habe gewusst, dass ich ka\\xcc\\x88me.'\n",
            " b'Tom said he knew that I would come.']\n",
            "[b'Tom ist nicht beru\\xcc\\x88hmt.' b'Tom is not famous.']\n",
            "[b'Glaubst du das alles?' b'Do you believe all that?']\n",
            "[b'Tom ist sich der Gefahr bewusst.' b'Tom is aware of the danger.']\n",
            "[b'Wa\\xcc\\x88ren Sie bitte so nett, mir kurz Ihr Handy auszuleihen?'\n",
            " b'Would you be so kind as to let me borrow your cell phone, please?']\n",
            "[b'Ihr mu\\xcc\\x88sst mehr u\\xcc\\x88ben!' b'You need to practice more.']\n",
            "[b'Ich habe von meinem Vater ein Buch bekommen.'\n",
            " b'I got a book from my father.']\n",
            "209199 26150 26150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXd6NoeTs_Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to txt"
      ],
      "metadata": {
        "id": "RrwlIufpnyAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with open(path + \"deu-eng-train.txt\", \"w\") as output:\n",
        "  for i, p in enumerate(train):\n",
        "    output.write(p[0].decode('utf-8') + '\\t' + p[1].decode('utf-8') + '\\n')\n",
        "\n",
        "with open(path + \"deu-eng-val.txt\", \"w\") as output:\n",
        "  for i, p in enumerate(val):\n",
        "    output.write(p[0].decode('utf-8') + '\\t' + p[1].decode('utf-8') + '\\n')\n",
        "\n",
        "with open(path + \"deu-eng-test.txt\", \"w\") as output:\n",
        "  for i, p in enumerate(test):\n",
        "    output.write(p[0].decode('utf-8') + '\\t' + p[1].decode('utf-8') + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOI9bXyWnxWa",
        "outputId": "0b2b2371-b582-4ea8-e612-ed9ffffacd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 912 ms, sys: 20.2 ms, total: 932 ms\n",
            "Wall time: 1.16 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to Pkl"
      ],
      "metadata": {
        "id": "PdUV4hbxAt_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "dump(train, open(path + \"deu-eng-train.pkl\", 'wb'))\n",
        "dump(val, open(path + \"deu-eng-val.pkl\", 'wb'))\n",
        "dump(test, open(path + \"deu-eng-test.pkl\", 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQePIPG9Avpo",
        "outputId": "7b7c5a8f-7e13-4833-b00d-ee4c28bf144c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 66.8 ms, sys: 527 ms, total: 593 ms\n",
            "Wall time: 2.12 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load from Pkl"
      ],
      "metadata": {
        "id": "6KUIvDlKKsHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'eng-deu-train.pkl'\n",
        "with open(path + filename, 'rb') as file:\n",
        "  dataset = load(file)\n",
        "for i, pair in enumerate(dataset):\n",
        "  if i == 3: break\n",
        "  print(pair[0])\n",
        "  print(pair[0].decode('utf-8'))\n",
        "  # print(pair[0].decode('utf-8'))\n",
        "  print(pair[1])\n",
        "  # print(pair[1].decode('utf-8'))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k42Q_k1wfRV-",
        "outputId": "de2aee8e-671d-4680-cc61-dc9a266bb61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Wartest du kurz?'\n",
            "Wartest du kurz?\n",
            "b'Will you wait a second?'\n",
            "\n",
            "\n",
            "b'Ich kann dir helfen, wenn du mich la\\xcc\\x88sst.'\n",
            "Ich kann dir helfen, wenn du mich lässt.\n",
            "b'I can help you if you let me.'\n",
            "\n",
            "\n",
            "b'Wann hast du das letztemal eine Eule gesehen?'\n",
            "Wann hast du das letztemal eine Eule gesehen?\n",
            "b'When was the last time you saw an owl?'\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = np.array(dataset)[:, 0]\n",
        "trainY = np.array(dataset)[:, 1]\n",
        "train_deu = tf.data.Dataset.from_tensor_slices(trainX)\n",
        "train_eng = tf.data.Dataset.from_tensor_slices(trainY)\n"
      ],
      "metadata": {
        "id": "aejI9JJjPRdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_deu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAgd-imLPRbZ",
        "outputId": "cab96cbe-407c-4400-a170-8876809fe0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, e in enumerate(train_deu):\n",
        "  if i == 3: break\n",
        "  print(e, type(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrli7M9SPRY3",
        "outputId": "1b0cd807-c54e-469f-ce98-9927c0924fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Wartest du kurz?', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(b'Ich kann dir helfen, wenn du mich la\\xcc\\x88sst.', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(b'Wann hast du das letztemal eine Eule gesehen?', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vocabulary"
      ],
      "metadata": {
        "id": "AteFuNK_LOtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_params = dict(lower_case=True)\n",
        "bert_tokenizer_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz6lYhPwfRTu",
        "outputId": "740dcac9-b6b0-4e66-f49f-4059290cdb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lower_case': True}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "reserved_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3YZtKIhLVhA",
        "outputId": "98932344-492a-40ed-fcbc-7ad0f41dcfba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]', '[UNK]', '[START]', '[END]']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_vocab_args = dict(\n",
        "    vocab_size = 8000,\n",
        "    reserved_tokens = reserved_tokens,\n",
        "    bert_tokenizer_params = bert_tokenizer_params,\n",
        "    learn_params = {}\n",
        ")\n",
        "bert_vocab_args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4av9QkJ9LVdx",
        "outputId": "31a98b37-8a59-4398-a086-eea3309196e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 8000,\n",
              " 'reserved_tokens': ['[PAD]', '[UNK]', '[START]', '[END]'],\n",
              " 'bert_tokenizer_params': {'lower_case': True},\n",
              " 'learn_params': {}}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "deu_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_deu.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zqhr8TBLVbR",
        "outputId": "1ef88659-a96e-4e71-e8c3-344cf88b410b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 19s, sys: 398 ms, total: 3min 19s\n",
            "Wall time: 3min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(deu_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtnFbPcJOmbR",
        "outputId": "82a90d86-47b9-46c8-a13a-a2a7de3c32c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(deu_vocab[:20])\n",
        "print(deu_vocab[100:120])\n",
        "print(deu_vocab[1000:1020])\n",
        "print(deu_vocab[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37c64y9YfRRW",
        "outputId": "f2df3b09-bab4-47de-ed06-00e72ad1257f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3']\n",
            "['haben', 'an', 'sind', 'kann', 'noch', 'einen', 'so', 'bin', 'von', 'dem', 'fur', 'als', 'dich', 'hast', 'sehr', 'sein', 'dir', 'hier', 'hatte', 'wurde']\n",
            "['mantel', 'acht', 'angekommen', 'betrunken', 'gitarre', 'schlaft', 'schreibtisch', 'ausland', 'freunden', 'gleiche', 'himmel', 'konne', 'passt', 'wert', 'ziehen', 'baby', 'eher', 'elke', 'fernseher', 'gespielt']\n",
            "['##/', '##:', '##;', '##?', '##j', '##q', '##°', '##ˋ', '##а', '##–', '##—', '##‘', '##’', '##‚', '##“', '##”', '##„', '##‟', '##‽', '##⁄']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)\n",
        "\n",
        "write_vocab_file(path + 'deu_vocab.txt', deu_vocab)"
      ],
      "metadata": {
        "id": "33uw94azfROG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_eng.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIrKvVEFRfBP",
        "outputId": "95017c8d-eabd-4d8e-ae88-982e967a6374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.6 s, sys: 85.7 ms, total: 48.7 s\n",
            "Wall time: 48.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng_vocab[:10])\n",
        "print(eng_vocab[100:110])\n",
        "print(eng_vocab[1000:1010])\n",
        "print(eng_vocab[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se0EXXYzRe_Y",
        "outputId": "192877af-26d3-4495-f2f7-bd5cad501527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", '(']\n",
            "['there', 'has', 'll', 've', 'here', 'very', 'go', 'think', 'about', 'didn']\n",
            "['earth', 'keys', 'lend', 'patient', 'sign', 'voice', 'forward', 'wears', 'accept', 'notice']\n",
            "['##?', '##j', '##q', '##°', '##–', '##—', '##’', '##“', '##”', '##€']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_vocab_file(path + 'eng_vocab.txt', eng_vocab)"
      ],
      "metadata": {
        "id": "Z5p_CByjRe83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Tokensizer"
      ],
      "metadata": {
        "id": "QpnWdzQbRtgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deu_tokenizer = tftxt.BertTokenizer(path + 'deu_vocab.txt', **bert_tokenizer_params)\n",
        "eng_tokenizer = tftxt.BertTokenizer(path + 'eng_vocab.txt', **bert_tokenizer_params)"
      ],
      "metadata": {
        "id": "-aVs0xGIRe4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en_examples is 1 batch of size 3.\n",
        "# it is a eagerTensor\n",
        "print(trainX[:3])\n",
        "\n",
        "print('\\n')\n",
        "token_batch = deu_tokenizer.tokenize(trainX[:3])\n",
        "print(token_batch.shape)\n",
        "print(token_batch)\n",
        "print(token_batch.to_list())\n",
        "\n",
        "print('\\n')\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "print(token_batch.shape)\n",
        "print(token_batch)\n",
        "print(token_batch.to_list())\n",
        "\n",
        "print('\\n')\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBdjQdCmVTOV",
        "outputId": "2343568a-3ff5-4e02-def3-2e33a2c20070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'Wartest du kurz?' b'Ich kann dir helfen, wenn du mich la\\xcc\\x88sst.'\n",
            " b'Wann hast du das letztemal eine Eule gesehen?']\n",
            "\n",
            "\n",
            "(3, None, None)\n",
            "<tf.RaggedTensor [[[2956], [74], [637], [28]],\n",
            " [[69], [103], [116], [213], [12], [130], [74], [95], [384], [14]],\n",
            " [[232], [113], [74], [73], [2215], [96], [33, 4005], [222], [28]]]>\n",
            "[[[2956], [74], [637], [28]], [[69], [103], [116], [213], [12], [130], [74], [95], [384], [14]], [[232], [113], [74], [73], [2215], [96], [33, 4005], [222], [28]]]\n",
            "\n",
            "\n",
            "(3, None)\n",
            "<tf.RaggedTensor [[2956, 74, 637, 28], [69, 103, 116, 213, 12, 130, 74, 95, 384, 14],\n",
            " [232, 113, 74, 73, 2215, 96, 33, 4005, 222, 28]]>\n",
            "[[2956, 74, 637, 28], [69, 103, 116, 213, 12, 130, 74, 95, 384, 14], [232, 113, 74, 73, 2215, 96, 33, 4005, 222, 28]]\n",
            "\n",
            "\n",
            "[2956, 74, 637, 28]\n",
            "[69, 103, 116, 213, 12, 130, 74, 95, 384, 14]\n",
            "[232, 113, 74, 73, 2215, 96, 33, 4005, 222, 28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "another_batch = [0, 1, 2, 3, 4, 5]\n",
        "txt_tokens = tf.gather(deu_vocab, another_batch)\n",
        "print(txt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv3FN-elZX58",
        "outputId": "bb86ac55-b4cb-4b15-e7f5-ca0fc304df01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'[PAD]' b'[UNK]' b'[START]' b'[END]' b'!' b'\"'], shape=(6,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "another_batch = [[1, 2, 3], [4, 5, 6], [7,8,9]]\n",
        "txt_tokens = tf.gather(deu_vocab, another_batch)\n",
        "print(type(txt_tokens))\n",
        "print(txt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRL8OAotZh52",
        "outputId": "365d6db2-8e11-4723-83f0-ce519459512a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(\n",
            "[[b'[UNK]' b'[START]' b'[END]']\n",
            " [b'!' b'\"' b'$']\n",
            " [b'%' b\"'\" b'(']], shape=(3, 3), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_tokens = tf.gather(deu_vocab, token_batch)\n",
        "print(txt_tokens)\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PhHF6tTVTMV",
        "outputId": "27deeb40-f74f-482f-8fc8-54505a0bdede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'wartest', b'du', b'kurz', b'?'],\n",
            " [b'ich', b'kann', b'dir', b'helfen', b',', b'wenn', b'du', b'mich',\n",
            "  b'lasst', b'.']                                                   ,\n",
            " [b'wann', b'hast', b'du', b'das', b'letztemal', b'eine', b'e', b'##ule',\n",
            "  b'gesehen', b'?']                                                      ]>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'wartest du kurz ?',\n",
              "       b'ich kann dir helfen , wenn du mich lasst .',\n",
              "       b'wann hast du das letztemal eine e ##ule gesehen ?'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = deu_tokenizer.detokenize(token_batch)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi9OywoOVTKO",
        "outputId": "37aeae49-7223-4f4a-ebd2-c5132cfd58a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'wartest', b'du', b'kurz', b'?'],\n",
            " [b'ich', b'kann', b'dir', b'helfen', b',', b'wenn', b'du', b'mich',\n",
            "  b'lasst', b'.']                                                   ,\n",
            " [b'wann', b'hast', b'du', b'das', b'letztemal', b'eine', b'eule',\n",
            "  b'gesehen', b'?']                                               ]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk6o6H6SVTH9",
        "outputId": "ae213119-28ed-4941-fc63-1e9e03e0fc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'wartest du kurz ?',\n",
              "       b'ich kann dir helfen , wenn du mich lasst .',\n",
              "       b'wann hast du das letztemal eine eule gesehen ?'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADD START, END"
      ],
      "metadata": {
        "id": "NJe-gMvzcGJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "print(START, END)\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  print(count)\n",
        "  starts = tf.fill([count, 1], START)\n",
        "  ends = tf.fill([count, 1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQWUPUepcIyE",
        "outputId": "9147bccc-76e9-4bab-a067-b94f0818fb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(3, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = eng_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZO8cf1mcItc",
        "outputId": "c55ee3aa-1d31-47b0-c84a-af323e10dc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(3, shape=(), dtype=int64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'[START] buses have song ? [END]',\n",
              "       b'[START] he ve up asked , when have at early . [END]',\n",
              "       b'[START] stay why have was nature she e babysitter leave ? [END]'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup detokenized text"
      ],
      "metadata": {
        "id": "ckqnoSihfIwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "bad_tokens_re = \"|\".join(bad_tokens)\n",
        "print(bad_tokens_re)\n",
        "token_txt = [\"[PAD]\", \"hello\", \"world\", \"[END]\"]\n",
        "bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
        "print(bad_cells)\n",
        "result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "print(result)\n",
        "result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvmrkoFt3ebP",
        "outputId": "347f151f-2161-4227-9664-e0739eab6936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\[PAD\\]|\\[START\\]|\\[END\\]\n",
            "tf.Tensor([ True False False  True], shape=(4,), dtype=bool)\n",
            "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
            "tf.Tensor(b'hello world', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_tokens_re = \"|\".join(bad_tokens)\n",
        "  #\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "QC5ufiNffIMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_examples = trainY[:3]\n",
        "print(eng_examples)\n",
        "print(eng_examples.dtype)\n",
        "eng_examples = eng_examples.astype('str')\n",
        "print(eng_examples)\n",
        "print(eng_examples.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtbJonWAfIKJ",
        "outputId": "d2bd2e73-489e-4403-ec33-8e4d9fbbc203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'Will you wait a second?' b'I can help you if you let me.'\n",
            " b'When was the last time you saw an owl?']\n",
            "|S537\n",
            "['Will you wait a second?' 'I can help you if you let me.'\n",
            " 'When was the last time you saw an owl?']\n",
            "<U537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_batch = eng_tokenizer.tokenize(eng_examples).merge_dims(-2,-1)\n",
        "words = eng_tokenizer.detokenize(token_batch)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBDOp94KcIrM",
        "outputId": "39a81a12-1c33-4e93-a63f-2a49fe4f5cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'will', b'you', b'wait', b'a', b'second', b'?'],\n",
              " [b'i', b'can', b'help', b'you', b'if', b'you', b'let', b'me', b'.'],\n",
              " [b'when', b'was', b'the', b'last', b'time', b'you', b'saw', b'an', b'owl',\n",
              "  b'?']                                                                    ]>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZXeqknG_B3t",
        "outputId": "029fd690-4342-4c1a-a42e-78ad9a4aaa30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'will you wait a second ?', b'i can help you if you let me .',\n",
              "       b'when was the last time you saw an owl ?'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export the Model"
      ],
      "metadata": {
        "id": "l0ikzXLoJ8hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = path + 'deu_vocab.txt'\n",
        "vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "# tf.Variable(vocab)\n",
        "print(type(vocab))\n",
        "tf.Variable(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT_S6y39C9_O",
        "outputId": "bc6a8d02-4445-45be-bdaf-11eabf090b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(7974,) dtype=string, numpy=\n",
              "array([b'[PAD]', b'[UNK]', b'[START]', ..., b'##\\xe2\\x80\\x9f',\n",
              "       b'##\\xe2\\x80\\xbd', b'##\\xe2\\x81\\x84'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = tftxt.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "    #\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create signatures for export\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    ##\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  ##\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    words = cleanup_text(self._reserved_tokens, words)\n",
        "    return words\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)\n"
      ],
      "metadata": {
        "id": "BYfshoBB_B2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.deu = CustomTokenizer(reserved_tokens, path + 'deu_vocab.txt')\n",
        "tokenizers.eng = CustomTokenizer(reserved_tokens, path + 'eng_vocab.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa9qsOiw_Bz1",
        "outputId": "9a42f357-a36b-4d3d-d56d-f78c7ae3afc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = path + 'tokenizer_deu_eng_1'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13XWMBSP_Bxo",
        "outputId": "c6ba8f99-fdea-463d-9de2-aa2703f4074b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Test The Model"
      ],
      "metadata": {
        "id": "Qal8tD0RKAZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
        "reloaded_tokenizers.deu.get_vocab_size().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvXKNOQe_BvW",
        "outputId": "13b1cf6c-4ab2-4592-9009-b66543fae88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7974"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = reloaded_tokenizers.deu.tokenize(['Ich habe mein Geld für Kleidung, Essen und Bücher ausgegeben.'])\n",
        "tokens.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7VqEWzS_BtG",
        "outputId": "ad2d310e-8777-4277-d345-a1fe9eec2e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2,   69,   86,  137,  194,  110, 1393,   12,  226,   97,  398,\n",
              "        2685,   14,    3]])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = reloaded_tokenizers.deu.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhnxYGDiLKXh",
        "outputId": "d517d144-248d-45a5-bdc4-aa14d17054c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'ich', b'habe', b'mein', b'geld', b'fur', b'kleidung',\n",
              "  b',', b'essen', b'und', b'bucher', b'ausgegeben', b'.', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round_trip = reloaded_tokenizers.deu.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IafPhjELKVK",
        "outputId": "3efd2e87-1b26-4dea-adc3-9cb0a6ac029d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ich habe mein geld fur kleidung , essen und bucher ausgegeben .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(path + 'tokenizer_deu_eng_1')\n",
        "string = \"When writing a sentence, generally you start with a capital letter and finish with a period (.), an exclamation mark (!), or a question mark (?).\"\n",
        "tokens = reloaded_tokenizers.eng.tokenize([string])\n",
        "tokens.numpy()\n",
        "text_tokens = reloaded_tokenizers.eng.lookup(tokens)\n",
        "text_tokens\n",
        "round_trip = reloaded_tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggp7OoaeyOmJ",
        "outputId": "da3a60f6-d3f7-4b6e-e314-2491900a7393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when writing a sentence , generally you start with a capital letter and finish with a period ( . ) , an exclamation mark ( ! ) , or a question mark ( ? ) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "Q9x0Uw9TLwZh"
      }
    }
  ]
}