{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Subword Tokenizer for Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw6cTrwKLtWB"
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge3c34n7ekNG"
   },
   "source": [
    "* tensorflow_datasets is needed to download data from google cloud storage (not used)\n",
    "* tensorflow_text is needed in the tokenizer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrVk3Z2odyp0",
    "outputId": "09a3ade2-1c1c-4259-cabe-3a9ba033477b"
   },
   "outputs": [],
   "source": [
    "# %pip install -q tensorflow_datasets\n",
    "# %pip install -q tensorflow_text tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G6_SGjB5fARd"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tftxt\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "from numpy.random import rand, shuffle\n",
    "import re\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhtZAKYjKmQa"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k42Q_k1wfRV-",
    "outputId": "de2aee8e-671d-4680-cc61-dc9a266bb61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Tom zahlt ein Bu\\xc3\\x9fgeld.'\n",
      "Tom zahlt ein Bußgeld.\n",
      "b'Tom is paying a fine.'\n",
      "Tom is paying a fine.\n",
      "\n",
      "\n",
      "b'Unter dem Bett ist eine Katze.'\n",
      "Unter dem Bett ist eine Katze.\n",
      "b'There is a cat under the bed.'\n",
      "There is a cat under the bed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/deu-eng-train.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "  dataset = load(file)\n",
    "for i, pair in enumerate(dataset):\n",
    "  if i == 2: break\n",
    "  print(pair[0])\n",
    "  print(pair[0].decode('utf-8'))\n",
    "  print(pair[1])\n",
    "  print(pair[1].decode('utf-8'))\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AteFuNK_LOtJ"
   },
   "source": [
    "# Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf Dataset datastrcuture\n",
    "The tf.data.Dataset.from_tensor_slices() method creates a dataset from a tensor or a list of tensors. The tensor or list of tensors is sliced along the first dimension, and the slices are the elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aejI9JJjPRdw"
   },
   "outputs": [],
   "source": [
    "trainX = np.array(dataset)[:, 0]\n",
    "trainY = np.array(dataset)[:, 1]\n",
    "train_deu = tf.data.Dataset.from_tensor_slices(trainX)\n",
    "train_eng = tf.data.Dataset.from_tensor_slices(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAgd-imLPRbZ",
    "outputId": "cab96cbe-407c-4400-a170-8876809fe0fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_deu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wrli7M9SPRY3",
    "outputId": "1b0cd807-c54e-469f-ce98-9927c0924fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Tom zahlt ein Bu\\xc3\\x9fgeld.', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'Unter dem Bett ist eine Katze.', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'Tom wollte, dass ich lu\\xcc\\x88ge.', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(train_deu):\n",
    "  if i == 3: break\n",
    "  print(e, type(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vz6lYhPwfRTu",
    "outputId": "740dcac9-b6b0-4e66-f49f-4059290cdb06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower_case': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer_params = dict(lower_case=True)\n",
    "bert_tokenizer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3YZtKIhLVhA",
    "outputId": "98932344-492a-40ed-fcbc-7ad0f41dcfba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[START]', '[END]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "reserved_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4av9QkJ9LVdx",
    "outputId": "31a98b37-8a59-4398-a086-eea3309196e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 8000,\n",
       " 'reserved_tokens': ['[PAD]', '[UNK]', '[START]', '[END]'],\n",
       " 'bert_tokenizer_params': {'lower_case': True},\n",
       " 'learn_params': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vocab_args = dict(\n",
    "    vocab_size = 8000,\n",
    "    reserved_tokens = reserved_tokens,\n",
    "    bert_tokenizer_params = bert_tokenizer_params,\n",
    "    learn_params = {}\n",
    ")\n",
    "bert_vocab_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocab with bert_vocab.bert_vocab_from_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Zqhr8TBLVbR",
    "outputId": "1ef88659-a96e-4e71-e8c3-344cf88b410b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 49s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deu_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_deu.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtnFbPcJOmbR",
    "outputId": "82a90d86-47b9-46c8-a13a-a2a7de3c32c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(deu_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37c64y9YfRRW",
    "outputId": "f2df3b09-bab4-47de-ed06-00e72ad1257f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3']\n",
      "['maria', 'ihr', 'haben', 'an', 'sind', 'kann', 'noch', 'bin', 'einen', 'so', 'von', 'fur', 'hast', 'dem', 'als', 'sehr', 'sein', 'dir', 'dich', 'hier']\n",
      "['kleine', 'meinte', 'messer', 'schreibtisch', 'wartet', 'zukunft', 'freunden', 'nun', '##sten', 'durfen', 'fuhlte', 'kleinen', 'schlaft', 'tu', 'gespielt', 'ordnung', 'acht', 'angelegenheit', 'bekam', 'fern']\n",
      "['##;', '##?', '##@', '##j', '##q', '##°', '##ˋ', '##а', '##–', '##—', '##‘', '##’', '##‚', '##“', '##”', '##„', '##‟', '##‽', '##⁄', '##€']\n"
     ]
    }
   ],
   "source": [
    "print(deu_vocab[:20])\n",
    "print(deu_vocab[100:120])\n",
    "print(deu_vocab[1000:1020])\n",
    "print(deu_vocab[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIrKvVEFRfBP",
    "outputId": "95017c8d-eabd-4d8e-ae88-982e967a6374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.7 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_eng.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "se0EXXYzRe_Y",
    "outputId": "192877af-26d3-4495-f2f7-bd5cad501527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", '(']\n",
      "['how', 'there', 'has', 'll', 've', 'here', 'very', 'think', 'go', 'about']\n",
      "['seeing', 'anywhere', 'suddenly', 'top', 'uncle', '##ment', 'common', 'earlier', 'keys', 'fault']\n",
      "['##j', '##q', '##v', '##°', '##—', '##‘', '##’', '##“', '##”', '##€']\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab[:10])\n",
    "print(eng_vocab[100:110])\n",
    "print(eng_vocab[1000:1010])\n",
    "print(eng_vocab[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "33uw94azfROG"
   },
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w', encoding='utf-8') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Z5p_CByjRe83"
   },
   "outputs": [],
   "source": [
    "write_vocab_file('../data/deu_vocab.txt', deu_vocab)\n",
    "write_vocab_file('../data/eng_vocab.txt', eng_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpnWdzQbRtgN"
   },
   "source": [
    "# Build Tokensizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-aVs0xGIRe4X"
   },
   "outputs": [],
   "source": [
    "deu_tokenizer = tftxt.BertTokenizer('../data/deu_vocab.txt', **bert_tokenizer_params)\n",
    "eng_tokenizer = tftxt.BertTokenizer('../data/eng_vocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Tom zahlt ein Bu\\xc3\\x9fgeld.' b'Unter dem Bett ist eine Katze.'\n",
      " b'Tom wollte, dass ich lu\\xcc\\x88ge.']\n",
      "\n",
      "\n",
      "(3, None, None)\n",
      "<tf.RaggedTensor [[[72], [1844], [85], [422, 177, 3316], [14]],\n",
      " [[369], [113], [399], [74], [97], [502], [14]],\n",
      " [[72], [201], [12], [86], [71], [1812], [14]]]>\n",
      "[[[72], [1844], [85], [422, 177, 3316], [14]], [[369], [113], [399], [74], [97], [502], [14]], [[72], [201], [12], [86], [71], [1812], [14]]]\n",
      "\n",
      "\n",
      "(3, None)\n",
      "<tf.RaggedTensor [[72, 1844, 85, 422, 177, 3316, 14],\n",
      " [369, 113, 399, 74, 97, 502, 14],\n",
      " [72, 201, 12, 86, 71, 1812, 14]]>\n",
      "[[72, 1844, 85, 422, 177, 3316, 14], [369, 113, 399, 74, 97, 502, 14], [72, 201, 12, 86, 71, 1812, 14]]\n",
      "\n",
      "\n",
      "[72, 1844, 85, 422, 177, 3316, 14]\n",
      "[369, 113, 399, 74, 97, 502, 14]\n",
      "[72, 201, 12, 86, 71, 1812, 14]\n"
     ]
    }
   ],
   "source": [
    "# en_examples is 1 batch of size 3.\n",
    "# it is a eagerTensor\n",
    "print(trainX[:3])\n",
    "\n",
    "print('\\n')\n",
    "token_batch = deu_tokenizer.tokenize(trainX[:3])\n",
    "print(token_batch.shape)\n",
    "print(token_batch)\n",
    "print(token_batch.to_list())\n",
    "\n",
    "print('\\n')\n",
    "token_batch = token_batch.merge_dims(-2,-1)\n",
    "print(token_batch.shape)\n",
    "print(token_batch)\n",
    "print(token_batch.to_list())\n",
    "\n",
    "print('\\n')\n",
    "for ex in token_batch.to_list():\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iv3FN-elZX58",
    "outputId": "bb86ac55-b4cb-4b15-e7f5-ca0fc304df01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'[PAD]' b'[UNK]' b'[START]' b'[END]' b'!' b'\"'], shape=(6,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "another_batch = [0, 1, 2, 3, 4, 5]\n",
    "txt_tokens = tf.gather(deu_vocab, another_batch)\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRL8OAotZh52",
    "outputId": "365d6db2-8e11-4723-83f0-ce519459512a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[b'[UNK]' b'[START]' b'[END]']\n",
      " [b'!' b'\"' b'$']\n",
      " [b'%' b\"'\" b'(']], shape=(3, 3), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "another_batch = [[1, 2, 3], [4, 5, 6], [7,8,9]]\n",
    "txt_tokens = tf.gather(deu_vocab, another_batch)\n",
    "print(type(txt_tokens))\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PhHF6tTVTMV",
    "outputId": "27deeb40-f74f-482f-8fc8-54505a0bdede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'tom', b'zahlt', b'ein', b'bus', b'##s', b'##geld', b'.'],\n",
      " [b'unter', b'dem', b'bett', b'ist', b'eine', b'katze', b'.'],\n",
      " [b'tom', b'wollte', b',', b'dass', b'ich', b'luge', b'.']]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'tom zahlt ein bus ##s ##geld .',\n",
       "       b'unter dem bett ist eine katze .',\n",
       "       b'tom wollte , dass ich luge .'], dtype=object)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tokens = tf.gather(deu_vocab, token_batch)\n",
    "print(txt_tokens)\n",
    "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize and detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wi9OywoOVTKO",
    "outputId": "37aeae49-7223-4f4a-ebd2-c5132cfd58a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'tom', b'zahlt', b'ein', b'bussgeld', b'.'],\n",
      " [b'unter', b'dem', b'bett', b'ist', b'eine', b'katze', b'.'],\n",
      " [b'tom', b'wollte', b',', b'dass', b'ich', b'luge', b'.']]>\n"
     ]
    }
   ],
   "source": [
    "words = deu_tokenizer.detokenize(token_batch)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fk6o6H6SVTH9",
    "outputId": "ae213119-28ed-4941-fc63-1e9e03e0fc21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'tom zahlt ein bussgeld .', b'unter dem bett ist eine katze .',\n",
       "       b'tom wollte , dass ich luge .'], dtype=object)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJe-gMvzcGJj"
   },
   "source": [
    "## ADD [START], [END]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQWUPUepcIyE",
    "outputId": "9147bccc-76e9-4bab-a067-b94f0818fb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(3, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "print(START, END)\n",
    "\n",
    "def add_start_end(ragged):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  print(count)\n",
    "  starts = tf.fill([count, 1], START)\n",
    "  ends = tf.fill([count, 1], END)\n",
    "  return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZO8cf1mcItc",
    "outputId": "c55ee3aa-1d31-47b0-c84a-af323e10dc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'[START] do kissing are john lot bent . [END]',\n",
       "       b'[START] parents all until was she mean . [END]',\n",
       "       b'[START] do room , your in meaning . [END]'], dtype=object)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = eng_tokenizer.detokenize(add_start_end(token_batch))\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckqnoSihfIwT"
   },
   "source": [
    "## Cleanup detokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvmrkoFt3ebP",
    "outputId": "347f151f-2161-4227-9664-e0739eab6936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\[PAD\\]|\\[START\\]|\\[END\\]\n",
      "tf.Tensor([ True False False  True], shape=(4,), dtype=bool)\n",
      "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
      "tf.Tensor(b'hello world', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "bad_tokens_re = \"|\".join(bad_tokens)\n",
    "print(bad_tokens_re)\n",
    "token_txt = [\"[PAD]\", \"hello\", \"world\", \"[END]\"]\n",
    "bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "print(bad_cells)\n",
    "result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "print(result)\n",
    "result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "QC5ufiNffIMr"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_tokens_re = \"|\".join(bad_tokens)\n",
    "  #\n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtbJonWAfIKJ",
    "outputId": "d2bd2e73-489e-4403-ec33-8e4d9fbbc203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Tom is paying a fine.' b'There is a cat under the bed.'\n",
      " b'Tom wanted me to lie.']\n",
      "|S537\n",
      "['Tom is paying a fine.' 'There is a cat under the bed.'\n",
      " 'Tom wanted me to lie.']\n",
      "<U537\n"
     ]
    }
   ],
   "source": [
    "eng_examples = trainY[:3]\n",
    "print(eng_examples)\n",
    "print(eng_examples.dtype)\n",
    "eng_examples = eng_examples.astype('str')\n",
    "print(eng_examples)\n",
    "print(eng_examples.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBDOp94KcIrM",
    "outputId": "39a81a12-1c33-4e93-a63f-2a49fe4f5cfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'tom', b'is', b'paying', b'a', b'fine', b'.'],\n",
       " [b'there', b'is', b'a', b'cat', b'under', b'the', b'bed', b'.'],\n",
       " [b'tom', b'wanted', b'me', b'to', b'lie', b'.']]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = eng_tokenizer.tokenize(eng_examples).merge_dims(-2,-1)\n",
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZXeqknG_B3t",
    "outputId": "029fd690-4342-4c1a-a42e-78ad9a4aaa30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'tom is paying a fine .', b'there is a cat under the bed .',\n",
       "       b'tom wanted me to lie .'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(reserved_tokens, words).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ikzXLoJ8hK"
   },
   "source": [
    "# Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mT_S6y39C9_O",
    "outputId": "bc6a8d02-4445-45be-bdaf-11eabf090b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7930,) dtype=string, numpy=\n",
       "array([b'[PAD]', b'[UNK]', b'[START]', ..., b'##\\xe2\\x80\\xbd',\n",
       "       b'##\\xe2\\x81\\x84', b'##\\xe2\\x82\\xac'], dtype=object)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_path = '../data/deu_vocab.txt'\n",
    "vocab = pathlib.Path(vocab_path).read_text(encoding=\"utf-8\").splitlines()\n",
    "# tf.Variable(vocab)\n",
    "print(type(vocab))\n",
    "tf.Variable(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "BYfshoBB_B2B"
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = tftxt.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "    #\n",
    "    vocab = pathlib.Path(vocab_path).read_text(encoding=\"utf-8\").splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create signatures for export\n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    ##\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  ##\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    words = cleanup_text(self._reserved_tokens, words)\n",
    "    return words\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa9qsOiw_Bz1",
    "outputId": "9a42f357-a36b-4d3d-d56d-f78c7ae3afc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.deu = CustomTokenizer(reserved_tokens, '../data/deu_vocab.txt')\n",
    "tokenizers.eng = CustomTokenizer(reserved_tokens, '../data/eng_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13XWMBSP_Bxo",
    "outputId": "c6ba8f99-fdea-463d-9de2-aa2703f4074b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
      "INFO:tensorflow:Assets written to: ./metadata/tokenizer_deu_eng\\assets\n"
     ]
    }
   ],
   "source": [
    "model_name = './metadata/tokenizer_deu_eng'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qal8tD0RKAZa"
   },
   "source": [
    "# Load and Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvXKNOQe_BvW",
    "outputId": "13b1cf6c-4ab2-4592-9009-b66543fae88a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7930"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.deu.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7VqEWzS_BtG",
    "outputId": "ad2d310e-8777-4277-d345-a1fe9eec2e98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   71,   88,  139,  194,  111, 1424,   12,  225,   99,  408,\n",
       "        3031,   14,    3]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.deu.tokenize(['Ich habe mein Geld für Kleidung, Essen und Bücher ausgegeben.'])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhnxYGDiLKXh",
    "outputId": "d517d144-248d-45a5-bdc4-aa14d17054c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'ich', b'habe', b'mein', b'geld', b'fur', b'kleidung',\n",
       "  b',', b'essen', b'und', b'bucher', b'ausgegeben', b'.', b'[END]']]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.deu.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IafPhjELKVK",
    "outputId": "3efd2e87-1b26-4dea-adc3-9cb0a6ac029d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich habe mein geld fur kleidung , essen und bucher ausgegeben .\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizers.deu.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggp7OoaeyOmJ",
    "outputId": "da3a60f6-d3f7-4b6e-e314-2491900a7393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when writing a sentence , generally you start with a capital letter and finish with a period ( . ) , an exclamation mark ( ! ) , or a question mark ( ? ) .\n"
     ]
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load('./metadata/tokenizer_deu_eng')\n",
    "string = \"When writing a sentence, generally you start with a capital letter and finish with a period (.), an exclamation mark (!), or a question mark (?).\"\n",
    "tokens = reloaded_tokenizers.eng.tokenize([string])\n",
    "tokens.numpy()\n",
    "text_tokens = reloaded_tokenizers.eng.lookup(tokens)\n",
    "text_tokens\n",
    "round_trip = reloaded_tokenizers.eng.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9x0Uw9TLwZh"
   },
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
